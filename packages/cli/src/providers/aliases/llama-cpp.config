{
  "name": "llama.cpp",
  "modelsDevProviderId": "llama",
  "baseProvider": "openai",
  "base-url": "http://localhost:8080/v1/",
  "defaultModel": "local-model",
  "description": "llama.cpp local compatibility profile"
}
