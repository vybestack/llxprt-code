# Phase 0.5a: Preflight Verification Results

## Phase ID

`PLAN-20251127-OPENAIVERCEL.P00.5a`

## Purpose

Document actual verification outputs before implementation begins.

---

## Dependency Verification

### ai (Vercel AI SDK Core)

```bash
npm ls ai
```

**Output**: 

```
@vybestack/llxprt-code@0.6.0 /Users/acoliver/projects/llxprt-code-branches/llxprt-code-3
├─┬ @vybestack/llxprt-code-core@0.6.0 -> ./packages/core
│ └── ai@4.3.19
└── ai@5.0.104
```

**Status**: [OK] Installed (Root: 5.0.104, Core package: 4.3.19)

**Notes**: Two versions present. Core package uses older 4.3.19.

### @ai-sdk/openai (OpenAI Provider for Vercel SDK)

```bash
npm ls @ai-sdk/openai
```

**Output**: 

```
@vybestack/llxprt-code@0.6.0 /Users/acoliver/projects/llxprt-code-branches/llxprt-code-3
├── @ai-sdk/openai@2.0.74
└─┬ @vybestack/llxprt-code-core@0.6.0 -> ./packages/core
  └── @ai-sdk/openai@1.3.24
```

**Status**: [OK] Installed (Root: 2.0.74, Core package: 1.3.24)

**Notes**: Two versions present. Core package uses older 1.3.24.

### vitest (Test Framework)

```bash
npm ls vitest
```

**Output**: 

```
├─┬ @fast-check/vitest@0.2.3
│ └── vitest@3.2.4 deduped
├─┬ @vitest/coverage-v8@3.2.4
│ └── vitest@3.2.4 deduped
├─┬ @vybestack/llxprt-code-a2a-server@0.6.0 -> ./packages/a2a-server
│ └── vitest@3.2.4 deduped
├─┬ @vybestack/llxprt-code-core@0.6.0 -> ./packages/core
│ └── vitest@3.2.4 deduped
└── vitest@3.2.4
```

**Status**: [OK] Installed (Version 3.2.4)

### typescript (TypeScript Compiler)

```bash
npm ls typescript
```

**Output**: 

```
└── typescript@5.8.3
```

**Status**: [OK] Installed (Version 5.8.3)

---

## Type/Interface Verification

### IProvider Interface

**Location**: `packages/core/src/providers/IProvider.ts`

**Definition**:

```typescript
export interface IProvider {
  name: string;
  defaultModel: string | null;
  models: IModel[];
  supportsSystemPrompt: boolean;
  supportsToolUse: boolean;
  supportsStreaming?: boolean;
  supportsNonStreaming?: boolean;
  toolFormat: ToolFormat;
  
  setRuntimeSettingsService(
    settingsService: SettingsService | null | undefined,
  ): void;
  
  getClient(options?: GenerateChatOptions): Promise<unknown>;
  
  generateChatCompletion(
    contents: IContent[],
    options?: GenerateChatOptions,
  ): AsyncIterableIterator<IContent>;
  
  getAvailableModels(): Promise<IModel[]>;
}
```

**Notes**: 
- Providers extend `BaseProvider` which implements `IProvider`
- The actual implementation method is `generateChatCompletionWithOptions()` (protected, overridden)
- Public `generateChatCompletion()` method wraps this and handles runtime context

### IContent Interface

**Location**: `packages/core/src/services/history/IContent.ts`

**Definition**:

```typescript
export interface IContent {
  speaker: 'human' | 'ai' | 'tool';
  blocks: ContentBlock[];
}

export type ContentBlock = 
  | TextBlock 
  | ToolCallBlock 
  | ToolResponseBlock 
  | ImageBlock 
  | DocumentBlock;

export interface TextBlock {
  type: 'text';
  text: string;
}

export interface ToolCallBlock {
  type: 'tool_call';
  id: string;
  name: string;
  arguments: string;
}

export interface ToolResponseBlock {
  type: 'tool_response';
  id: string;
  result: string;
}

export interface ImageBlock {
  type: 'image';
  data: string;
  mimeType: string;
}

export interface DocumentBlock {
  type: 'document';
  data: string;
  mimeType: string;
}
```

**Notes**: 
- This is the universal content representation
- Provider implementations convert from this to provider-specific formats
- No intermediate IMessage type used

### GenerateChatOptions Type

**Location**: `packages/core/src/providers/IProvider.ts`

**Definition**:

```typescript
export interface GenerateChatOptions {
  tools?: ITool[];
  model?: string;
  settings?: SettingsService;
  telemetry?: TelemetryService;
  config?: Config;
}

export interface NormalizedGenerateChatOptions {
  contents: IContent[];
  tools?: ITool[];
  model: string;
  resolved: {
    telemetry: TelemetryService | null;
    runtimeKey: string;
  };
  settings?: SettingsService;
  config?: Config;
}
```

**Notes**:
- `GenerateChatOptions` is what users pass
- `NormalizedGenerateChatOptions` is what providers receive in `generateChatCompletionWithOptions()`
- BaseProvider handles the normalization

### ITool Interface

**Location**: `packages/core/src/providers/ITool.ts`

**Definition**:

```typescript
export interface ITool {
  function: {
    name: string;
    description: string;
    parameters: Record<string, unknown>;
  };
}
```

**Notes**: 
- OpenAI-compatible format
- Providers convert to their native format as needed

---

## Pattern Verification

### Provider Implementation Pattern

All providers follow this pattern:

1. **Extend BaseProvider**: `class XProvider extends BaseProvider implements IProvider`
2. **Constructor**: Accept API keys, base URLs, config, OAuth manager
3. **Override generateChatCompletionWithOptions()**: Main implementation method
4. **Conversion Methods**: Convert IContent to provider-specific message format
5. **Streaming**: Use async generator pattern with `yield`

### Streaming Implementation

**From OpenAIProvider.ts (lines 596-750)**:

```typescript
protected override async *generateChatCompletionWithOptions(
  options: NormalizedGenerateChatOptions,
): AsyncIterableIterator<IContent> {
  const client = await this.getClient(options);
  
  // Convert IContent to provider messages
  const messages = this.convertToOpenAIMessages(options.contents);
  
  // Create streaming request
  const stream = await client.chat.completions.create({
    model: options.model,
    messages,
    tools: convertedTools,
    stream: true,
  });
  
  // Process stream and yield IContent
  for await (const chunk of stream) {
    const content = this.convertChunkToIContent(chunk);
    yield content;
  }
}
```

**Key Points**:
- Use `async *` generator syntax
- Convert input `IContent[]` to provider format before calling API
- Convert streaming response back to `IContent` before yielding
- Handle tool calls and responses in the stream

### Message Conversion Pattern

**From OpenAIProvider.ts (lines 787-900)**:

```typescript
private convertToOpenAIMessages(
  contents: IContent[],
  mode: ToolReplayMode = 'native',
  config?: Config,
): OpenAI.Chat.ChatCompletionMessageParam[] {
  const messages: OpenAI.Chat.ChatCompletionMessageParam[] = [];
  
  for (const content of contents) {
    if (content.speaker === 'human') {
      const textBlocks = content.blocks.filter(b => b.type === 'text');
      const text = textBlocks.map(b => b.text).join('\n');
      messages.push({ role: 'user', content: text });
    }
    else if (content.speaker === 'ai') {
      // Convert tool calls, text, etc.
      const toolCalls = content.blocks.filter(b => b.type === 'tool_call');
      const textBlocks = content.blocks.filter(b => b.type === 'text');
      // ... build assistant message
    }
    else if (content.speaker === 'tool') {
      // Convert tool responses
    }
  }
  
  return messages;
}
```

**Key Points**:
- Extract blocks by type from `IContent.blocks`
- Map speaker types: `human` → `user`, `ai` → `assistant`, `tool` → `tool`
- Handle multiple blocks per content (e.g., text + tool calls)
- Each provider has its own conversion method

### Tool Conversion Pattern

**From OpenAIProvider.ts**:

```typescript
// Tools are passed in ITool format (OpenAI-compatible)
const { tools } = options;

// Convert to OpenAI SDK format
const openaiTools = tools?.map(tool => ({
  type: 'function' as const,
  function: tool.function,
}));

// Pass to API
const response = await client.chat.completions.create({
  tools: openaiTools,
  // ...
});
```

**Key Points**:
- `ITool` format is already OpenAI-compatible
- Just wrap in `{ type: 'function', function: tool.function }`
- Other providers (like Anthropic) convert differently

### Error Handling Pattern

**Common across providers**:

```typescript
try {
  const stream = await client.chat.completions.create({...});
  for await (const chunk of stream) {
    yield convertChunk(chunk);
  }
} catch (error) {
  if (error.response?.status === 401) {
    throw new AuthenticationError('Invalid API key');
  }
  throw error;
}
```

**Key Points**:
- Handle API errors gracefully
- Convert to framework-specific error types when needed
- Let BaseProvider handle runtime context errors

### Provider Registration Pattern

**From packages/cli/src/providers/providerManagerInstance.ts (lines 350-450)**:

```typescript
// In createProviderManager():

// 1. Create provider instance
const openaiProvider = new OpenAIProvider(
  apiKey,
  baseUrl,
  providerConfig,
  oauthManager,
);

// 2. Register with manager
manager.registerProvider(openaiProvider);

// 3. Register OAuth provider if needed
void ensureOAuthProviderRegistered(
  'openai',
  oauthManager,
  tokenStore,
  addItem,
);
```

**Key Points**:
- Providers instantiated with config + OAuth manager
- Registered via `manager.registerProvider()`
- OAuth registration is separate step
- Provider names must be unique

---

## File Structure Verification

### Test File Locations and Naming

Based on existing provider structure:

```
packages/core/src/providers/openai-vercel/
├── __tests__/
│   ├── streaming.test.ts
│   └── nonStreaming.test.ts
├── OpenAIVercelProvider.test.ts
├── OpenAIVercelProvider.ts
├── messageConversion.test.ts
├── messageConversion.ts
├── toolIdUtils.test.ts
├── toolIdUtils.ts
└── index.ts
```

**Patterns**:
- Main provider tests: `<ProviderName>.test.ts` 
- Integration tests in `__tests__/` subdirectory
- Utility/conversion logic in separate files with tests
- Barrel export via `index.ts`

### Provider File Structure

**Minimum Files Required**:

1. **Provider Implementation**: `OpenAIVercelProvider.ts`
   - Main provider class
   - Extends BaseProvider
   - Implements generateChatCompletionWithOptions()

2. **Main Test File**: `OpenAIVercelProvider.test.ts`
   - Basic provider tests (name, models, etc.)
   - Authentication tests
   - Interface compliance tests

3. **Message Conversion**: `messageConversion.ts` (optional but recommended)
   - Convert IContent to Vercel AI format
   - Separate from main provider for testability

4. **Message Conversion Tests**: `messageConversion.test.ts`
   - Test all conversion scenarios
   - Test edge cases

5. **Integration Tests**: `__tests__/streaming.test.ts`, `__tests__/nonStreaming.test.ts`
   - End-to-end streaming tests
   - End-to-end non-streaming tests

6. **Barrel Export**: `index.ts`
   - Export provider class
   - Export any public types/utilities

---

## Discrepancies and Blockers

### [OK] Dependencies

**Status**: All dependencies are installed.

**Notes**:
- `ai` package has two versions (4.3.19 in core, 5.0.104 in root)
- `@ai-sdk/openai` has two versions (1.3.24 in core, 2.0.74 in root)
- Core package should use the versions in its own package.json
- This is expected in a monorepo setup

### [OK] Type Definitions

**Status**: All required types are defined and documented.

**Key Findings**:
- No IMessage intermediate type - providers convert directly from IContent
- GenerateChatOptions is normalized to NormalizedGenerateChatOptions by BaseProvider
- All providers override `generateChatCompletionWithOptions()` not `generateChatCompletion()`

### [OK] Implementation Patterns

**Status**: Clear patterns established and documented.

**Key Insights**:
- Message conversion happens in provider-specific methods
- Streaming uses async generator pattern
- Tool conversion is provider-specific
- Registration happens in CLI package's providerManagerInstance.ts

### WARNING: Existing Implementation

**Status**: Provider already exists at `packages/core/src/providers/openai-vercel/`

**Files Found**:
- OpenAIVercelProvider.ts (already implemented)
- OpenAIVercelProvider.test.ts (tests exist)
- messageConversion.ts (conversion logic exists)
- messageConversion.test.ts (conversion tests exist)
- toolIdUtils.ts (utility exists)
- toolIdUtils.test.ts (utility tests exist)
- __tests__/streaming.test.ts (streaming tests exist)
- __tests__/nonStreaming.test.ts (non-streaming tests exist)

**Impact**: 
- Provider implementation is already complete
- Tests are already written
- May need verification that implementation matches requirements
- May need to check if provider is registered in CLI

###  Next Steps Required

1. **Verify Registration**: Check if OpenAIVercelProvider is registered in `packages/cli/src/providers/providerManagerInstance.ts`
2. **Run Existing Tests**: Verify all tests pass
3. **Compare with Requirements**: Verify implementation matches all requirements from PLAN-ORIGINAL.md
4. **Update Documentation**: Ensure all documentation is current

---

## Summary

All dependencies are installed and all type definitions are documented. Clear implementation patterns have been identified from existing providers. 

**Critical Finding**: The OpenAIVercelProvider already exists with full implementation and tests. This preflight verification reveals that the provider has already been implemented, and the next phase should focus on verification, registration, and documentation rather than new implementation.
