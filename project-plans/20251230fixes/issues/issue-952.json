{"body":"### What happened?\n\n‚úì  ReadFile integration-tests/test-helper.ts\n\n ‚ä∑  ReadFile ...integration-tests//json-output.test.ts\n\n    Running: ...integration-tests//json-output.test.ts\n\n ‚úì  SearchText 'output-format|outputFormat|--output-format|--outputFormat|json-output' in integration-tests/**/*.ts within ./\n\n    No matches found\n\nThe read file has been going on for a while and normally is instant. So far I've only seen this with gpt-5.2. so either it is some tool thing with Responses or is a timing issue that gpt seems to hit more often\n\n### What did you expect to happen?\n\nnot hang\n\n### Client information\n\nv0.7.0-nightly.251223.e40c645db\n\n### Login information\n\n_No response_\n\n### Anything else we need to know?\n\n_No response_","coderabbit_comments":[{"author":"coderabbitai","body":"\u003c!-- This is an auto-generated issue plan by CodeRabbit --\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eüîó Similar Issues\u003c/b\u003e\u003c/summary\u003e\n\n**Possible Duplicates**\n- https://github.com/vybestack/llxprt-code/issues/152\n\n**Related Issues**\n- https://github.com/vybestack/llxprt-code/issues/104\n- https://github.com/vybestack/llxprt-code/issues/951\n- https://github.com/vybestack/llxprt-code/issues/84\n- https://github.com/vybestack/llxprt-code/issues/231\n\u003c/details\u003e\n\u003cdetails\u003e\n\u003csummary\u003e\u003cb\u003eüë§ Suggested Assignees\u003c/b\u003e\u003c/summary\u003e\n\n- [acoliver](https://github.com/acoliver)\n- [dipping5115](https://github.com/dipping5115)\n\u003c/details\u003e\n\n\n---\n\u003cdetails\u003e\n\u003csummary\u003eEnable issue planning\u003c/summary\u003e\n\n\u003csub\u003eTo enable issue planning, add the following to your `.coderabbit.yaml`:\u003c/sub\u003e\n```yaml\nissue_enrichment:\n  planning:\n    enabled: true\n```\n\n\u003csub\u003eYou can then request a plan by commenting `@coderabbitai plan` on any issue.\u003c/sub\u003e\n\u003c/details\u003e\n\n\u003cdetails\u003e\n\u003csummary\u003e üß™ Issue enrichment is currently in open beta.\u003c/summary\u003e\n\n\nYou can configure auto-planning by selecting labels in the issue_enrichment configuration.\n\nTo disable automatic issue enrichment, add the following to your `.coderabbit.yaml`:\n```yaml\nissue_enrichment:\n  auto_enrich:\n    enabled: false\n```\n\u003c/details\u003e\n\nüí¨ Have feedback or questions? Drop into our [discord](https://discord.gg/coderabbit) or [schedule a call](https://calendly.com/parth-coderabbit/issues-feedback)!","createdAt":"2025-12-23T21:12:38Z","url":"https://github.com/vybestack/llxprt-code/issues/952#issuecomment-3687979076"},{"author":"coderabbitai","body":"\u003c!-- This is an auto-generated reply by CodeRabbit --\u003e\n\u003e [!TIP]\n\u003e For best results, initiate chat on the files or code changes.\n\n\u003cdetails\u003e\n\u003csummary\u003eüß© Analysis chain\u003c/summary\u003e\n\nüèÅ Script executed:\n```\n#!/bin/bash\n\n# First, let's find files related to tool execution and ReadFile\necho \"=== Finding tool execution files ===\"\nfd -e ts -e js | rg -i \"(tool|execute)\" | head -30\n\necho -e \"\\n=== Finding ReadFile implementation ===\"\nrg -n \"class ReadFile|ReadFile.*Tool|readfile\" --type=ts -i | head -20\n\necho -e \"\\n=== Finding parallel tool execution logic ===\"\nrg -n \"parallel.*tool|executeTools|executeTool\" --type=ts -C3 | head -50\n```\n\nLength of output: 7388\n\n---\n\nüèÅ Script executed:\n```\n#!/bin/bash\n\necho \"=== Finding ReadFile tool class ===\"\nrg -n \"class.*ReadFile|export.*ReadFile\" --type=ts -A5 | head -40\n\necho -e \"\\n=== Finding tool execution in scheduler ===\"\ncat packages/ui/src/hooks/useToolScheduler.ts\n\necho -e \"\\n=== Looking for tool execution completion logic ===\"\nrg -n \"toolComplete|tool.*complete|onComplete\" --type=ts -C3 | head -60\n```\n\nLength of output: 19478\n\n---\n\nüèÅ Script executed:\n```\n#!/bin/bash\n\necho \"=== Finding CoreToolScheduler implementation ===\"\nfd \"CoreToolScheduler.ts\" --type f\n\necho -e \"\\n=== Reading CoreToolScheduler ===\"\nrg -n \"class CoreToolScheduler\" --type=ts -A10\n\necho -e \"\\n=== Looking for schedule method and execution logic ===\"\nast-grep --pattern 'class CoreToolScheduler {\n  $$$\n  schedule($$$) {\n    $$$\n  }\n  $$$\n}'\n```\n\nLength of output: 1149\n\n---\n\nüèÅ Script executed:\n```\n#!/bin/bash\n\necho \"=== Reading full CoreToolScheduler file ===\"\ncat packages/core/src/core/coreToolScheduler.ts\n```\n\nLength of output: 51027\n\n---\n\nüèÅ Script executed:\n```\n#!/bin/bash\n\necho \"=== Checking recent git history for parallel tool changes ===\"\ngit log --oneline --all -20 --grep=\"parallel\\|tool\\|scheduler\" -- packages/core/src/core/coreToolScheduler.ts\n\necho -e \"\\n=== Checking recent commits to coreToolScheduler.ts ===\"\ngit log --oneline -10 -- packages/core/src/core/coreToolScheduler.ts\n\necho -e \"\\n=== Looking for recent changes to publishBufferedResults ===\"\ngit log -p -5 --all -- packages/core/src/core/coreToolScheduler.ts | grep -A30 -B5 \"publishBufferedResults\"\n```\n\nLength of output: 2438\n\n---\n\nüèÅ Script executed:\n```\n#!/bin/bash\n\necho \"=== Getting details of the buffered parallel tool execution commit ===\"\ngit show 9039042c --stat\n\necho -e \"\\n=== Getting the actual changes to publishBufferedResults ===\"\ngit show 9039042c -- packages/core/src/core/coreToolScheduler.ts | grep -A50 \"publishBufferedResults\"\n\necho -e \"\\n=== Checking if there's a lock or guard mechanism ===\"\nrg -n \"isPublishing|publishLock|publishing\" packages/core/src/core/coreToolScheduler.ts\n```\n\nLength of output: 5855\n\n---\n\nüèÅ Script executed:\n```\n#!/bin/bash\n\necho \"=== Looking at the test file for parallel execution tests ===\"\nrg -n \"parallel.*execution|buffered.*result\" packages/core/src/core/coreToolScheduler.test.ts -A10 | head -80\n\necho -e \"\\n=== Checking if tests cover concurrent completion ===\"\nrg -n \"concurrent|race|timing\" packages/core/src/core/coreToolScheduler.test.ts -C3\n```\n\nLength of output: 1951\n\n---\n\nüèÅ Script executed:\n````\n#!/bin/bash\n\necho \"=== Creating a minimal reproduction test case ===\"\ncat \u003e /tmp/race_condition_demo.md \u003c\u003c 'EOF'\n# Race Condition in publishBufferedResults\n\n## The Problem\n\nWhen multiple tools complete in parallel, they each call `publishBufferedResults()` concurrently.\nThe function filters `this.toolCalls` each time it runs:\n\n```typescript\nconst callsInOrder = this.toolCalls.filter(\n  (call) =\u003e call.status === 'scheduled' || call.status === 'executing',\n);\n```\n\nAs tools get published, their status changes from 'executing' to 'success', so they get\nfiltered OUT on subsequent calls. But `nextPublishIndex` is shared and assumes a stable array.\n\n## Example Scenario (3 parallel tools: A, B, C)\n\n### Initial state:\n- All 3 tools scheduled and start executing\n- nextPublishIndex = 0\n- toolCalls = [A(executing), B(executing), C(executing)]\n\n### Tool B completes first (fastest):\n- Buffers result, calls publishBufferedResults()\n- callsInOrder = [A, B, C] (all still 'executing')\n- nextPublishIndex = 0\n- Checks A at index 0: not ready, breaks\n- nextPublishIndex stays 0\n\n### Tool A completes second:\n- Buffers result, calls publishBufferedResults()\n- callsInOrder = [A, B, C] (might be [A, B, C] or some may be 'success')\n- nextPublishIndex = 0\n- Publishes A at index 0 ‚Üí status becomes 'success', nextPublishIndex = 1\n- Publishes B at index 1 ‚Üí status becomes 'success', nextPublishIndex = 2\n- Checks C at index 2: not ready, breaks\n\n### Tool C completes last:\n- Buffers result, calls publishBufferedResults()\n- callsInOrder = filter(status === 'scheduled' || 'executing')\n  - A is 'success' ‚Üí FILTERED OUT\n  - B is 'success' ‚Üí FILTERED OUT  \n  - C is 'executing' ‚Üí INCLUDED\n- callsInOrder = [C] (length = 1)\n- nextPublishIndex = 2 (from previous calls)\n- while (2 \u003c 1) ‚Üí FALSE, loop never runs\n- **C's result is never published!**\n\nThe tool appears to hang because the result sits in pendingResults forever.\n\n## Root Cause\n\nThe array `callsInOrder` is recalculated on each call based on current status,\nbut `nextPublishIndex` assumes a stable array. When already-published tools\nget filtered out, the array shrinks, making nextPublishIndex point beyond\nthe array bounds.\nEOF\n\ncat /tmp/race_condition_demo.md\n\necho -e \"\\n=== Verifying the filtering logic in the actual code ===\"\nrg -n \"callsInOrder.*filter\" packages/core/src/core/coreToolScheduler.ts -A2 -B2\n````\n\nLength of output: 2404\n\n---\n\n\n\n\u003c/details\u003e\n\n@acoliver I found the root cause of the hanging parallel tool calls! It's a **race condition** in the `publishBufferedResults()` method introduced in commit `9039042c`.\n\n## The Problem\n\nWhen multiple tools execute in parallel and complete around the same time, they each call `publishBufferedResults()` concurrently. The method recalculates its working array on every invocation:\n\n```typescript\n// Line 1356-1358 in coreToolScheduler.ts\nconst callsInOrder = this.toolCalls.filter(\n  (call) =\u003e call.status === 'scheduled' || call.status === 'executing',\n);\n```\n\nAs tools get published, their status changes from `'executing'` ‚Üí `'success'`, so they're **filtered out** on subsequent calls. But `nextPublishIndex` is shared state that assumes a stable array.\n\n## Race Condition Scenario\n\nLet's say 3 tools (A, B, C) execute in parallel:\n\n1. **All tools start executing**: `nextPublishIndex = 0`, all have status `'executing'`\n\n2. **Tool B completes first** (fastest):\n   - Calls `publishBufferedResults()`\n   - `callsInOrder = [A, B, C]` (all still executing)\n   - Checks index 0 (Tool A): not ready yet, breaks\n   - `nextPublishIndex` stays 0\n\n3. **Tool A completes second**:\n   - Calls `publishBufferedResults()`\n   - `callsInOrder = [A, B, C]`\n   - Publishes A at index 0 ‚Üí status becomes `'success'`, `nextPublishIndex = 1`\n   - Publishes B at index 1 ‚Üí status becomes `'success'`, `nextPublishIndex = 2`\n   - Checks C at index 2: not ready, breaks\n\n4. **Tool C completes last**:\n   - Calls `publishBufferedResults()`\n   - `callsInOrder = filter(...)` ‚Üí **A and B filtered out (status='success')**\n   - `callsInOrder = [C]` (length = 1)\n   - `while (2 \u003c 1)` ‚Üí **FALSE, loop never runs!**\n   - **Tool C's result never gets published** ‚ùå\n\nThe tool appears to hang because its result sits in `pendingResults` forever.\n\n## The Fix\n\nAdd a concurrency guard to ensure only one `publishBufferedResults()` executes at a time:\n\n```typescript\nprivate isPublishingResults = false;\n\nprivate async publishBufferedResults(signal: AbortSignal): Promise\u003cvoid\u003e {\n  // Guard against concurrent execution\n  if (this.isPublishingResults) {\n    return;\n  }\n  \n  this.isPublishingResults = true;\n  try {\n    const callsInOrder = this.toolCalls.filter(\n      (call) =\u003e call.status === 'scheduled' || call.status === 'executing',\n    );\n\n    // Publish results in original request order\n    while (this.nextPublishIndex \u003c callsInOrder.length) {\n      const expectedCall = callsInOrder[this.nextPublishIndex];\n      const buffered = this.pendingResults.get(expectedCall.request.callId);\n\n      if (!buffered) {\n        // Next result not ready yet, stop publishing\n        break;\n      }\n\n      // Publish this result\n      await this.publishResult(buffered, signal);\n\n      // Remove from buffer\n      this.pendingResults.delete(buffered.callId);\n      this.nextPublishIndex++;\n    }\n\n    // Check if all tools completed\n    if (\n      this.nextPublishIndex === callsInOrder.length \u0026\u0026\n      callsInOrder.length \u003e 0\n    ) {\n      // Reset for next batch\n      this.nextPublishIndex = 0;\n      this.pendingResults.clear();\n    }\n  } finally {\n    this.isPublishingResults = false;\n  }\n}\n```\n\nThis ensures the filtered array doesn't shrink underneath the publishing loop, preventing the index from going out of bounds.\n\n## Why This Hits GPT-5.2 More Often\n\nThe issue is timing-dependent. GPT-5.2 likely has different response patterns or uses more parallel tool calls, hitting the race condition window more frequently than other models.\n\nWould you like me to open an issue for this, or would you prefer to create a fix PR directly?\n\n---\n\n\u003cdetails\u003e\n\u003csummary\u003eüß† Learnings used\u003c/summary\u003e\n\n```\nLearnt from: e2720pjk\nRepo: vybestack/llxprt-code PR: 583\nFile: packages/core/src/providers/openai/OpenAIProvider.ts:935-959\nTimestamp: 2025-11-16T22:51:26.374Z\nLearning: In the llxprt-code codebase (packages/core/src/providers/openai/OpenAIProvider.ts), tools like `run_shell_command` use internal streaming only for real-time UI updates during execution, but each tool execution produces exactly ONE final `ToolResponseBlock` containing the full result. The streaming chunks are never sent to the LLM and are not converted into multiple tool messages. The OpenAI Chat Completions API requires that each tool call (tool_call_id) corresponds to exactly one message with role 'tool', so duplicate tool response detection that removes subsequent tool messages with the same tool_call_id is correct and necessary for API compliance.\n```\n\n\u003c/details\u003e\n\n","createdAt":"2025-12-23T21:15:15Z","url":"https://github.com/vybestack/llxprt-code/issues/952#issuecomment-3687985254"},{"author":"coderabbitai","body":"\u003c!-- This is an auto-generated reply by CodeRabbit --\u003e\n\u003e [!TIP]\n\u003e For best results, initiate chat on the files or code changes.\n\n\u003cdetails\u003e\n\u003csummary\u003eüß© Analysis chain\u003c/summary\u003e\n\nüèÅ Script executed:\n```\n#!/bin/bash\n\n# Fetch details of issue #957\necho \"=== Fetching issue #957 details ===\"\ngh issue view 957 --repo vybestack/llxprt-code --json number,title,body,state,createdAt,author,comments --jq '{number: .number, title: .title, body: .body, state: .state, createdAt: .createdAt, author: .author.login, comments: [.comments[] | {author: .author.login, body: .body, createdAt: .createdAt}]}'\n```\n\nLength of output: 64889\n\n---\n\n\n\n\u003c/details\u003e\n\n@acoliver Yes, absolutely! **These are the same bug** ‚Äî just different symptoms of the race condition in `publishBufferedResults()`.\n\n## The Connection\n\n**Root Cause (identified in #957):**\nThe `publishBufferedResults()` method is called concurrently by multiple tool completion handlers when parallel tools finish simultaneously, causing a race condition where `nextPublishIndex` can be corrupted and tools hang in 'executing' state forever.\n\n**How they manifest differently:**\n\n**Issue #957** - User sees the **queue error**:\n- Tools hang in 'executing' state due to race\n- Subsequent tool calls queue up waiting\n- User hits Escape to cancel\n- Queued items abort with \"Tool call cancelled while in queue\"\n\n**Issue #952** - User sees the **hang itself**:\n- ReadFile (or any tool) appears to hang\n- Actually completed successfully, but status never updated from 'executing' to 'success'\n- No queue error visible yet because user hasn't tried to cancel\n- Just looks like the tool is \"taking forever for no reason\"\n\n## Why #952 Happens More with GPT-5.2\n\nGPT-5.2 likely:\n- Makes more parallel tool calls\n- Has faster response times, causing tools to complete closer together\n- Hits the race condition window more frequently\n\nThe race happens when tools finish within microseconds of each other (very common with fast operations like ReadFile on SSD), and both simultaneously call `publishBufferedResults()` from their completion handlers (line 1510 in coreToolScheduler.ts).\n\n## The Fix\n\nAs identified in the #957 discussion, `publishBufferedResults()` needs to be made reentrant-safe with a mutex/lock to serialize concurrent calls. This will fix both:\n- #952: Tools won't hang anymore ‚úì\n- #957: No more queue buildup ‚Üí no more \"cancelled while in queue\" errors ‚úì\n\nWant me to open a single PR that fixes both issues by adding the mutex to `publishBufferedResults()`?\n\n---\n\n\u003cdetails\u003e\n\u003csummary\u003eüß† Learnings used\u003c/summary\u003e\n\n```\nLearnt from: e2720pjk\nRepo: vybestack/llxprt-code PR: 583\nFile: packages/core/src/providers/openai/OpenAIProvider.ts:935-959\nTimestamp: 2025-11-16T22:51:26.374Z\nLearning: In the llxprt-code codebase (packages/core/src/providers/openai/OpenAIProvider.ts), tools like `run_shell_command` use internal streaming only for real-time UI updates during execution, but each tool execution produces exactly ONE final `ToolResponseBlock` containing the full result. The streaming chunks are never sent to the LLM and are not converted into multiple tool messages. The OpenAI Chat Completions API requires that each tool call (tool_call_id) corresponds to exactly one message with role 'tool', so duplicate tool response detection that removes subsequent tool messages with the same tool_call_id is correct and necessary for API compliance.\n```\n\n---\n\n```\nLearnt from: acoliver\nRepo: vybestack/llxprt-code PR: 0\nFile: :0-0\nTimestamp: 2025-12-26T14:17:41.974Z\nLearning: In packages/core/src/core/coreToolScheduler.ts, the `publishBufferedResults()` method (line 1355) is called concurrently by multiple tool completion handlers (line 1510 in each tool's `.then()` callback) when parallel tools finish simultaneously. This causes a race condition where `nextPublishIndex` can be corrupted and tools can hang in 'executing' state forever, blocking the scheduler and causing subsequent tool calls to queue indefinitely. The method must be made reentrant-safe using a mutex/lock pattern to serialize concurrent calls.\n```\n\n---\n\n```\nLearnt from: acoliver\nRepo: vybestack/llxprt-code PR: 0\nFile: :0-0\nTimestamp: 2025-12-18T14:06:22.557Z\nLearning: OpenAIResponsesProvider (packages/core/src/providers/openai-responses/OpenAIResponsesProvider.ts) currently bypasses the ephemeral truncation system by using direct `JSON.stringify(toolResponseBlock.result)` and needs to be updated to support ephemeral settings like the other providers.\n```\n\n\u003c/details\u003e\n\n","createdAt":"2025-12-29T19:19:26Z","url":"https://github.com/vybestack/llxprt-code/issues/952#issuecomment-3697309301"}],"number":952,"title":"sometimes parallel tool calls are hanging"}
